{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b06f641c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "ffmpeg_bin_path = r\"C:\\Users\\arezk\\AppData\\Local\\Microsoft\\WinGet\\Packages\\Gyan.FFmpeg_Microsoft.Winget.Source_8wekyb3d8bbwe\\ffmpeg-8.0.1-full_build\\bin\"\n",
    "os.environ[\"PATH\"] += os.pathsep + ffmpeg_bin_path\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "563c5bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import spectrogram_to_audio\n",
    "from src.model import Unet\n",
    "import torch \n",
    "import numpy as np\n",
    "import IPython.display as ipd\n",
    "import musdb\n",
    "import librosa\n",
    "from scipy.io import wavfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68a95a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the first test spectrogram\n",
    "\n",
    "mixture_path = '../spectrograms/musdb18/test/mixture/Al_James_-_Schoolboy_Facination_spec.npy'\n",
    "vocal_path = '../spectrograms/musdb18/test/vocal/Al_James_-_Schoolboy_Facination_spec.npy'\n",
    "mix_phase_path= '../spectrograms/musdb18/test/phase/Al_James_-_Schoolboy_Facination_phase.npy'\n",
    "\n",
    "# Load the first one\n",
    "mix_spec = np.load(mixture_path)\n",
    "vocal_spec = np.load(vocal_path)\n",
    "mix_phase=np.load(mix_phase_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "233852a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from ../checkpoints/musdb18_V2/model_last.pth\n",
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model = Unet()\n",
    "model_path = '../checkpoints/musdb18_V2/model_last.pth'\n",
    "model.load(model_path)\n",
    "model_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0882c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total frames to process: 2137\n"
     ]
    }
   ],
   "source": [
    "# Inference parameters\n",
    "patch_size = 128\n",
    "hop_size = 64  # 50% overlap \n",
    "\n",
    "# Get dimensions\n",
    "freq_bins, total_frames = mix_spec.shape\n",
    "print(f\"Total frames to process: {total_frames}\")\n",
    "\n",
    "# Normalize per-song\n",
    "norm = mix_spec.max()\n",
    "mix_normalized = mix_spec / norm\n",
    "\n",
    "vocal_spec_sum = np.zeros_like(mix_spec)\n",
    "weight_sum = np.zeros(total_frames)\n",
    "\n",
    "# Sliding window inference\n",
    "start = 0\n",
    "patch_count = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "882bd5cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10 patches, frames 576-704/2137\n",
      "Processed 20 patches, frames 1216-1344/2137\n",
      "Processed 30 patches, frames 1856-1984/2137\n",
      "\n",
      "Total patches processed: 34\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    while start < total_frames:\n",
    "        end = min(start + patch_size, total_frames)\n",
    "        \n",
    "        # Extract patch\n",
    "        patch = mix_normalized[:, start:end]\n",
    "        original_patch_size = patch.shape[1]\n",
    "        \n",
    "        # Pad if necessary \n",
    "        if patch.shape[1] < patch_size:\n",
    "            padding = patch_size - patch.shape[1]\n",
    "            patch = np.pad(patch, ((0, 0), (0, padding)), mode='constant')\n",
    "        \n",
    "        # Remove first frequency bin (513 → 512)\n",
    "        patch_512 = patch[1:, :]\n",
    "        \n",
    "        # Convert to tensor \n",
    "        patch_tensor = torch.from_numpy(patch_512[np.newaxis, np.newaxis, :, :]).float()\n",
    "        patch_tensor = patch_tensor.to(model_device)\n",
    "        \n",
    "        # Predict mask\n",
    "        mask = model.forward(patch_tensor)\n",
    "        \n",
    "        # Convert back to numpy\n",
    "        mask_np = mask.cpu().numpy()[0, 0, :, :]\n",
    "        \n",
    "        # Add first frequency bin back (512 → 513)\n",
    "        mask_full = np.zeros((513, patch_size))\n",
    "        mask_full[1:, :] = mask_np\n",
    "        \n",
    "        # Apply mask to patch\n",
    "        vocal_patch = mask_full * patch\n",
    "        \n",
    "        # remove padding\n",
    "        vocal_patch = vocal_patch[:, :original_patch_size]\n",
    "        \n",
    "        # Accumulate in full spectrogram\n",
    "        vocal_spec_sum[:, start:end] += vocal_patch\n",
    "        weight_sum[start:end] += 1\n",
    "        \n",
    "        patch_count += 1\n",
    "        if patch_count % 10 == 0:\n",
    "            print(f\"Processed {patch_count} patches, frames {start}-{end}/{total_frames}\")\n",
    "        \n",
    "        start += hop_size\n",
    "\n",
    "print(f\"\\nTotal patches processed: {patch_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35668e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final vocal spec shape: (513, 2137)\n",
      "Vocal spec range: [0.000, 1.000]\n",
      "Reconstructed audio shape: (8831025,)\n",
      "Saved predicted vocals to: ../outputs/Al_James_-_Schoolboy_Facination_vocals_predicted.wav\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Average overlapping regions\n",
    "vocal_spec_full = vocal_spec_sum / weight_sum[np.newaxis, :]\n",
    "\n",
    "# Denormalize\n",
    "vocal_spec_full = vocal_spec_full * norm\n",
    "\n",
    "print(f\"Final vocal spec shape: {vocal_spec_full.shape}\")\n",
    "print(f\"Vocal spec range: [{vocal_spec_full.min():.3f}, {vocal_spec_full.max():.3f}]\")\n",
    "\n",
    "# Reconstruct audio\n",
    "vocal_audio_predicted = spectrogram_to_audio(vocal_spec_full, mix_phase)\n",
    "\n",
    "print(f\"Reconstructed audio shape: {vocal_audio_predicted.shape}\")\n",
    "\n",
    "# save the audio\n",
    "output_path = '../outputs/Al_James_-_Schoolboy_Facination_vocals_predicted.wav'\n",
    "\n",
    "wavfile.write(output_path, 44100, vocal_audio_predicted)\n",
    "print(f\"Saved predicted vocals to: {output_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fc0f1e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: Al James - Schoolboy Facination\n",
      "Duree: 200.327007\n",
      "Shape of mix: (8835072,)\n",
      "Shape vocals: (8835072,)\n",
      "Sample rate: 44100\n"
     ]
    }
   ],
   "source": [
    "# load test track the same as the spectrogram above\n",
    "\n",
    "mus_test = musdb.DB(root='../data/musdb18', is_wav=False, subsets='test')\n",
    "ground_truth_track = mus_test[1] \n",
    "\n",
    "\n",
    "rate = ground_truth_track.rate\n",
    "mix_audio = ground_truth_track.audio  \n",
    "vocal_audio = ground_truth_track.targets['vocals'].audio  \n",
    "\n",
    "\n",
    "mix_mono = np.mean(mix_audio, axis=1)  \n",
    "vocal_mono = np.mean(vocal_audio, axis=1)  \n",
    "\n",
    "\n",
    "print(\"name:\", ground_truth_track.name)\n",
    "print(\"Duree:\", ground_truth_track.duration)\n",
    "print(\"Shape of mix:\", mix_mono.shape)  #  stéréo\n",
    "print(\"Shape vocals:\", vocal_mono.shape)\n",
    "print(\"Sample rate:\", ground_truth_track.rate) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "593a72d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth Mix:\n",
      "Ground Truth Vocals:\n",
      "predicted vocal\n"
     ]
    }
   ],
   "source": [
    "# display the mix and vocals from the ground truth track and the pridected vocal\n",
    "\n",
    "print(\"Ground Truth Mix:\")\n",
    "# display(ipd.Audio(ground_truth_track.audio.T, rate=ground_truth_track.rate))\n",
    "\n",
    "print(\"Ground Truth Vocals:\")\n",
    "# display(ipd.Audio(ground_truth_track.targets['vocals'].audio.T, rate=ground_truth_track.rate))\n",
    "\n",
    "print(\"predicted vocal\")\n",
    "# display(ipd.Audio(vocal_audio_predicted, rate=ground_truth_track.rate))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Unet_tp_son",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
